{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業目標]\n",
    "持續接觸有關機器學習的相關專案與最新技術"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業重點]\n",
    "透過觀察頂尖公司的機器學習文章，來了解各公司是怎麼應用機器學習在實際的專案上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [作業]\n",
    "今天的作業希望大家能夠看看全球機器學習巨頭們在做的機器學習專案。以 google 為例，下圖是 Google 內部專案使用機器學習的數量，隨著時間進展，現在早已超過 2000 個專案在使用機器學習。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://cdn-images-1.medium.com/max/800/1*U_L8qI8RmYS-MOBrYvXhSA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "底下幫同學整理幾間知名企業的 blog 或機器學習網站 (自行搜尋也可)，這些網站都會整理最新的機器學習專案或者是技術文章，請挑選一篇文章閱讀並試著回答\n",
    "1. 專案的目標？ (要解決什麼問題）\n",
    "2. 使用的技術是？ (只需知道名稱即可，例如：使用 CNN 卷積神經網路做影像分類)\n",
    "3. 資料來源？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Google AI blog](https://ai.googleblog.com/)\n",
    "- [Facebook Research blog](https://research.fb.com/blog/)\n",
    "- [Apple machine learning journal](https://machinelearning.apple.com/)\n",
    "- [機器之心](https://www.jiqizhixin.com/)\n",
    "- [雷鋒網](http://www.leiphone.com/category/ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "\n",
    "挑選的文章是這篇：\n",
    "\n",
    "## [Exploring Transfer Learning with T5: the Text-To-Text Transfer Transformer](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)\n",
    "\n",
    "- - -\n",
    "\n",
    "## 作業 1：目標\n",
    "\n",
    "建立一個全新的model [\"Text-To-Text Transfer Transformer (T5)\"](https://arxiv.org/abs/1910.10683) 和引進 [\"Colossal Clean Crawled Corpus (C4)\"](https://www.tensorflow.org/datasets/catalog/c4) 這個open-source的pre-training dataset，\n",
    "\n",
    "並在目前眾多的NLP benchmarks上面取得state-of-the-art的好成績，\n",
    "\n",
    "希望在所有的scenario中可以用同一個已經pre-train過的model，不改動loss function, and hyperparameters的情況下，可以只作fine-tuned便達成downstream的NLP task，\n",
    "\n",
    "這邊的task包括了machine translation, document summarization, question answering, and classification tasks (e.g., sentiment analysis)......等等所有常見的NLP應用，也可以用在regression問題上，並且將output以string representation來表示。\n",
    "\n",
    "- - -\n",
    "\n",
    "## 作業 2：方法\n",
    "\n",
    "GOOGLE提出一個全新的framework稱為 \"Text-To-Text\" ，有別於以往的seq2seq，\n",
    "\n",
    "text即為一個強調各element間順序的sequence，element不一定是word，\n",
    "\n",
    "此model從input到output都是string，克服了以往BERT-style models只能output class label或span of the input的難處。\n",
    "\n",
    "在實驗過程中，為了pre-training時所用的unlabeled dataset，GOOGLE希望這個dataset能同時達到not only high quality and diverse, but also massive，\n",
    "\n",
    "以往從維基百科上爬資料，雖然高品質但style都過於相近，不具有多樣性；而Common Crawl提供的dataset，品質又過於參差不齊。\n",
    "\n",
    "為此GOOGLE自己整理出了一套稱為 \"Colossal Clean Crawled Corpus (C4)\" 的語料庫(corpus)。\n",
    "\n",
    "整理過往世界上常用過的Transfer Learning的方法，分析如下：\n",
    "\n",
    "* model architectures, where we found that encoder-decoder models generally outperformed \"decoder-only\" language models;\n",
    "\n",
    "    (encoder-decoder優於只有decoder的models)\n",
    "\n",
    "\n",
    "* pre-training objectives, where we confirmed that fill-in-the-blank-style denoising objectives (where the model is trained to recover missing words in the input) worked best and that the most important factor was the computational cost;\n",
    "\n",
    "    (pre-training以填空降噪為目標時效果最好，而且最重要的因素是計算成本。)\n",
    "\n",
    "\n",
    "* unlabeled datasets, where we showed that training on in-domain data can be beneficial but that pre-training on smaller datasets can lead to detrimental overfitting;\n",
    "\n",
    "    (對於in-domain的data，用unlabeled datasets進行pre-training可能有好處，但是在較小的datasets上可能會overfitting。)\n",
    "\n",
    "\n",
    "* training strategies, where we found that multitask learning could be close to competitive with a pre-train-then-fine-tune approach but requires carefully choosing how often the model is trained on each task;\n",
    "\n",
    "    (訓練策略上用multitask learning可以利用pre-train-then-fine-tune的方法，但是必須謹慎選擇在每個task上訓練的頻率。)\n",
    "\n",
    "\n",
    "* and scale, where we compare scaling up the model size, the training time, and the number of ensembled models to determine how to make the best use of fixed compute power.\n",
    "\n",
    "    (按比例縮放模型大小、調整訓練時間和ensembled models的數量，找到最能充分利用computer power的方法。)\n",
    "\n",
    "\n",
    "T5便是如此誕生的，based on transfer learning、誇張的參數量(11 billion parameters，比BERT LARGE的3.4 billion大上不少)\n",
    "\n",
    "和大規模乾淨的corpus，以及適合的調參、資料過濾等等策略。\n",
    "\n",
    "透過T5將Text-To-Text的framework對每個task整合，便得到了在眾多corpus上達到state-of-the-art的成果。\n",
    "\n",
    "- - -\n",
    "\n",
    "## 作業 3：資料來源\n",
    "\n",
    "[Common Crawl](https://commoncrawl.org)利用自己的網絡爬蟲收集了PB級別的網頁數據，\n",
    "\n",
    "而C4這個datasets則是更進一步從Common Crawl上面清理資料而來，\n",
    "\n",
    "清理過程包含刪除重複數據、丟棄不完整的句子且刪除一些令人反感的內容；\n",
    "\n",
    "經整理之後，變成一個足夠乾淨且比維基還要大上兩倍的dataset。\n",
    "\n",
    "最終作者宣稱C4用在transfer learning上，可以使downstream tasks有更好的結果，而且在pre-training時較不容易overfitting。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}