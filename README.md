# 100Day-ML-Marathon

![coverPhoto](images/coverPhoto.jpg?raw=true)

## 每日練習內容
| DAY | 練習內容 | 完成？ |
|:---:| :------: | :----: |
| 1 | 資料介紹與評估資料 (申論+程式碼) | Y |
| 2 | 機器學習概論 (申論題) | Y |
| 3 | 機器學習-流程與步驟 (申論題) | Y |
| 4 | EDA/讀取資料與分析流程 | Y |
| 5 | 如何新建一個 dataframe? 如何讀取其他資料? (非 csv 的資料) | Y |
| 6 | EDA: 欄位的資料類型介紹及處理 | Y |
| 7 | 特徵類型 | Y |
| 8 | EDA資料分佈 | Y |
| 9 | EDA: Outlier 及處理 | Y |
| 10 | 數值型特徵 - 去除離群值 | Y |
| 11 | 常用的數值取代：中位數與分位數連續數值標準化 | Y |
| 12 | 數值型特徵-補缺失值與標準化 | Y |
| 13 | DataFrame operationData frame merge/常用的 DataFrame 操作 | Y |
| 14 | 程式實作 EDA: correlation/相關係數簡介 | Y |
| 15 | EDA from Correlation | Y |
| 16 | EDA: 不同數值範圍間的特徵如何檢視/繪圖與樣式Kernel Density Estimation (KDE) | Y |
| 17 | EDA: 把連續型變數離散化 | Y |
| 18 | 程式實作 把連續型變數離散化 | Y |
| 19 | Subplots | Y |
| 20 | Heatmap & Grid-plot | Y |
| 21 | 模型初體驗 Logistic Regression | Y |
| 22 | 特徵工程簡介 | N |
| 23 | 數值型特徵 - 去除偏態 | N |
| 24 | 類別型特徵 - 基礎處理 | N |
| 25 | 類別型特徵 - 均值編碼 | N |
| 26 | 類別型特徵 - 其他進階處理 | N |
| 27 | 時間型特徵 | N |
| 28 | 特徵組合 - 數值與數值組合 | N |
| 29 | 特徵組合 - 類別與數值組合 | N |
| 30 | 特徵選擇 | N |
| 31 | 特徵評估 | N |
| 32 | 分類型特徵優化 - 葉編碼 | N |
| 33 | 機器如何學習? | N |
| 34 | 訓練/測試集切分的概念 | N |
| 35 | regression vs. classification | N |
| 36 | 評估指標選定/evaluation metrics | N |
| 37 | regression model 介紹 - 線性迴歸/羅吉斯回歸 | N |
| 38 | regression model 程式碼撰寫 | N |
| 39 | regression model 介紹 - LASSO 回歸/ Ridge 回歸 | N |
| 40 | regression model 程式碼撰寫 | N |
| 41 | tree based model - 決策樹 (Decision Tree) 模型介紹 | N |
| 42 | tree based model - 決策樹程式碼撰寫 | N |
| 43 | tree based model - 隨機森林 (Random Forest) 介紹 | N |
| 44 | tree based model - 隨機森林程式碼撰寫 | N |
| 45 | tree based model - 梯度提升機 (Gradient Boosting Machine) 介紹 | N |
| 46 | tree based model - 梯度提升機程式碼撰寫 | N |
| 47 | 超參數調整與優化 | N |
| 48 | Kaggle 競賽平台介紹 | N |
| 49 | 集成方法 : 混合泛化(Blending) | N |
| 50 | 集成方法 : 堆疊泛化(Stacking) | N |
| 51 | Kaggle期中考 考ML與調參相關(1) | N |
| 52 | Kaggle期中考 考ML與調參相關(2) | N |
| 53 | Kaggle期中考 考ML與調參相關(3) | N |
| 54 | clustering 1 非監督式機器學習簡介 | N |
| 55 | clustering 2 聚類算法 | N |
| 56 | K-mean 觀察 : 使用輪廓分析 | N |
| 57 | clustering 3 階層分群算法 | N |
| 58 | 階層分群法 觀察 : 使用 2D 樣版資料集 | N |
| 59 | dimension reduction 1 降維方法-主成份分析 | N |
| 60 | PCA 觀察 : 使用手寫辨識資料集 | N |
| 61 | dimension reduction 2 降維方法-T-SNE | N |
| 62 | t-sne 觀察 : 分群與流形還原 | N |
| 63 | 神經網路介紹 | N |
| 64 | 深度學習體驗 : 模型調整與學習曲線 | N |
| 65 | 深度學習體驗 : 啟動函數與正規化 | N |
| 66 | Keras 安裝與介紹 | N |
| 67 | Keras Dataset | N |
| 68 | Keras Sequential API | N |
| 69 | Keras Module API | N |
| 70 | Multi-layer Perception多層感知 | N |
| 71 | 損失函數 | N |
| 72 | 啟動函數 | N |
| 73 | 梯度下降Gradient Descent | N |
| 74 | Gradient Descent 數學原理 | N |
| 75 | BackPropagation | N |
| 76 | 優化器optimizers | N |
| 77 | 訓練神經網路的細節與技巧 - Validation and overfit | N |
| 78 | 訓練神經網路前的注意事項 | N |
| 79 | 訓練神經網路的細節與技巧 - Learning rate effect | N |
| 80 | [練習 Day] 優化器與學習率的組合與比較 | N |
| 81 | 訓練神經網路的細節與技巧 - Regularization | N |
| 82 | 訓練神經網路的細節與技巧 - Dropout | N |
| 83 | 訓練神經網路的細節與技巧 - Batch normalization | N |
| 84 | [練習 Day] 正規化/機移除/批次標準化的 組合與比較 | N |
| 85 | 訓練神經網路的細節與技巧 - 使用 callbacks 函數做 earlystop | N |
| 86 | 訓練神經網路的細節與技巧 - 使用 callbacks 函數儲存 model | N |
| 87 | 訓練神經網路的細節與技巧 - 使用 callbacks 函數做 reduce learning rate | N |
| 88 | 訓練神經網路的細節與技巧 - 撰寫自己的 callbacks 函數 | N |
| 89 | 訓練神經網路的細節與技巧 - 撰寫自己的 Loss function | N |
| 90 | 使用傳統電腦視覺與機器學習進行影像辨識 | N |
| 91 | [練習 Day] 使用傳統電腦視覺與機器學習進行影像辨識 | N |
| 92 | 卷積神經網路 (Convolution Neural Network, CNN) 簡介 | N |
| 93 | 卷積神經網路架構細節 | N |
| 94 | 卷積神經網路 - 卷積(Convolution)層與參數調整 | N |
| 95 | 卷積神經網路 - 池化(Pooling)層與參數調整 | N |
| 96 | Keras 中的 CNN layers | N |
| 97 | 使用 CNN 完成 CIFAR-10 資料集 | N |
| 98 | 訓練卷積神經網路的細節與技巧 - 處理大量數據 | N |
| 99 | 訓練卷積神經網路的細節與技巧 - 處理小量數據 | N |
| 100 | 訓練卷積神經網路的細節與技巧 - 轉移學習 (Transfer learning) | N |